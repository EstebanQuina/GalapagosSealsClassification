{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnUS3VNfag7wsbjyFX6/zx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"df21a39da343477a961f1d6804b0186c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e79a5dcda684e739251df57c1958931","IPY_MODEL_efd194df348b4618bb34ee804c5e11ee","IPY_MODEL_a6e5294d96d142389576289c6f00a29b"],"layout":"IPY_MODEL_b0acc0b50d4045efae4efab460532dab"}},"7e79a5dcda684e739251df57c1958931":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a350894c6744d8b9866cd8cbc332dfd","placeholder":"​","style":"IPY_MODEL_b5494a39c360448fafb68a6ecec68d27","value":"Downloading Images: 100%"}},"efd194df348b4618bb34ee804c5e11ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b135e2b213447af894ab7cfa0aecc98","max":592,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9695d134a21d4009bc54bf92fd332531","value":592}},"a6e5294d96d142389576289c6f00a29b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77c3569ce2ef4111912011288ff35363","placeholder":"​","style":"IPY_MODEL_e9fd95b6c2114f6796ffe21064cd8dd0","value":" 592/592 [00:06&lt;00:00, 67.51it/s]"}},"b0acc0b50d4045efae4efab460532dab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a350894c6744d8b9866cd8cbc332dfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5494a39c360448fafb68a6ecec68d27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b135e2b213447af894ab7cfa0aecc98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9695d134a21d4009bc54bf92fd332531":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77c3569ce2ef4111912011288ff35363":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9fd95b6c2114f6796ffe21064cd8dd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"362092aa05ed42d595bd169e3cd12ef1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d91b7115b9cc45db802f423b27e690a4","IPY_MODEL_b370744d19a44870bba58034fa2c1356","IPY_MODEL_ed40693bd152422b85a2302f2fa898eb"],"layout":"IPY_MODEL_1c836158cda34019917de085d00be4f0"}},"d91b7115b9cc45db802f423b27e690a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dfcbebf45c547c78c9ae89b0557aeca","placeholder":"​","style":"IPY_MODEL_fcf83ecb71e347d99fdde23781965a13","value":"Downloading Images: 100%"}},"b370744d19a44870bba58034fa2c1356":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce99d00526664b8badd83be183f1e312","max":6793,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ebfe9b88f234007ab505b82bfd49e84","value":6793}},"ed40693bd152422b85a2302f2fa898eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e60271962da34a50ade1df25b7243cc7","placeholder":"​","style":"IPY_MODEL_6c315d3173aa41f7b72c1da1eda7c7c1","value":" 6793/6793 [00:30&lt;00:00, 264.83it/s]"}},"1c836158cda34019917de085d00be4f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dfcbebf45c547c78c9ae89b0557aeca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf83ecb71e347d99fdde23781965a13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce99d00526664b8badd83be183f1e312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ebfe9b88f234007ab505b82bfd49e84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e60271962da34a50ade1df25b7243cc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c315d3173aa41f7b72c1da1eda7c7c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Step 1: Mount Google Drive (if not already mounted)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Define your download directory in Drive\n","import os\n","import zipfile\n","import pandas as pd\n","import requests\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm.notebook import tqdm # For a nice progress bar\n","\n","# Define the main directory in your Drive for all GBIF downloads\n","gbif_root_dir = '/content/drive/My Drive/gbif_raw_downloads2'\n","os.makedirs(gbif_root_dir, exist_ok=True)\n","\n","# Define a specific directory for the Galapagos Fur Seal dataset\n","dataset_name = 'Arctocephalus_galapagoensis_DwC-A'\n","dataset_drive_path = os.path.join(gbif_root_dir, dataset_name)\n","os.makedirs(dataset_drive_path, exist_ok=True)\n","print(f\"Dataset will be processed in: {dataset_drive_path}\")\n","\n","# --- IMPORTANT: Paste your GBIF Download Link here ---\n","GBIF_DOWNLOAD_URL = 'https://api.gbif.org/v1/occurrence/download/request/0010447-250515123054153.zip'\n","\n","\n","dwca_filename = os.path.join(dataset_drive_path, 'Arctocephalus_galapagoensis_dataset.zip')\n","\n","print(f\"Downloading DwC-A to: {dwca_filename}\")\n","# Download the zip file directly to your Drive\n","# The -O flag saves the file with the specified name\n","!wget -O \"{dwca_filename}\" \"{GBIF_DOWNLOAD_URL}\"\n","\n","print(\"\\nDwC-A download complete. Starting extraction...\")\n","\n","# Unzip the downloaded DwC-A directly in Drive\n","try:\n","    with zipfile.ZipFile(dwca_filename, 'r') as zip_ref:\n","        zip_ref.extractall(dataset_drive_path)\n","    print(\"DwC-A extracted successfully.\")\n","except zipfile.BadZipFile:\n","    print(\"Error: The downloaded file is not a valid zip file. Please check the GBIF_DOWNLOAD_URL.\")\n","    exit()\n","except Exception as e:\n","    print(f\"An error occurred during unzipping: {e}\")\n","    exit()\n","\n","# Path to the multimedia.txt file within the extracted archive\n","multimedia_file = os.path.join(dataset_drive_path, 'multimedia.txt')\n","\n","if not os.path.exists(multimedia_file):\n","    print(f\"Error: {multimedia_file} not found. Ensure the DwC-A contained it.\")\n","    print(\"This usually means no media records were found for your search criteria on GBIF.\")\n","else:\n","    # Read the multimedia.txt file into a Pandas DataFrame\n","    multimedia_df = pd.read_csv(multimedia_file, sep='\\t', low_memory=False)\n","\n","    print(f\"Loaded {len(multimedia_df)} records from multimedia.txt\")\n","    print(\"\\nColumns available in multimedia.txt:\")\n","    print(multimedia_df.columns.tolist()) # <--- IMPORTANT: Print available columns\n","\n","    # Determine which column to use for organizing images\n","    # We prefer 'taxonKey', then 'scientificName', otherwise a generic folder\n","    image_label_column = None\n","    if 'taxonKey' in multimedia_df.columns:\n","        image_label_column = 'taxonKey'\n","    elif 'scientificName' in multimedia_df.columns: # As a fallback for organization\n","        image_label_column = 'scientificName'\n","    else:\n","        print(\"\\nWarning: Neither 'taxonKey' nor 'scientificName' found in multimedia.txt.\")\n","        print(\"Images will be saved to a single folder without species-specific subfolders.\")\n","\n","    # Filter for 'StillImage' (photos) and relevant columns\n","    # 'identifier' is the image URL\n","    # 'license' is crucial for legal use\n","    required_cols = ['identifier', 'license', 'type']\n","    if image_label_column:\n","        required_cols.append(image_label_column)\n","\n","    # Ensure all required columns exist before selecting\n","    missing_cols = [col for col in required_cols if col not in multimedia_df.columns]\n","    if missing_cols:\n","        print(f\"Error: Missing required columns in multimedia.txt: {missing_cols}\")\n","        print(\"Please check your GBIF download settings or the file content.\")\n","        exit()\n","\n","    image_data = multimedia_df[\n","        (multimedia_df['type'] == 'StillImage') &\n","        (multimedia_df['identifier'].notna())\n","    ][required_cols].dropna(subset=['identifier'])\n","\n","    # Filter to only keep images for the specific taxonKey if 'taxonKey' is present\n","    ARCTOCEPHALUS_GALAPAGOENSIS_TAXON_KEY = 2433473\n","    if image_label_column == 'taxonKey':\n","        image_data = image_data[image_data['taxonKey'] == ARCTOCEPHALUS_GALAPAGOENSIS_TAXON_KEY]\n","        print(f\"Found {len(image_data)} 'StillImage' URLs for TaxonKey {ARCTOCEPHALUS_GALAPAGOENSIS_TAXON_KEY}.\")\n","    elif image_label_column == 'scientificName':\n","        # If using scientificName, you might want to filter by the name as well\n","        image_data = image_data[image_data['scientificName'] == 'Arctocephalus galapagoensis']\n","        print(f\"Found {len(image_data)} 'StillImage' URLs for scientificName 'Arctocephalus galapagoensis'.\")\n","    else:\n","        print(f\"Found {len(image_data)} 'StillImage' URLs (no specific taxonKey filter applied).\")\n","\n","\n","    if len(image_data) == 0:\n","        print(\"No images found matching your criteria in the multimedia.txt file.\")\n","    else:\n","        # Create a directory to store the actual image files\n","        image_output_dir = os.path.join(dataset_drive_path, 'images')\n","        os.makedirs(image_output_dir, exist_ok=True)\n","\n","        # Define the final image saving directory\n","        if image_label_column:\n","            # Create a subfolder based on taxonKey or scientificName\n","            # Make sure to convert label to string for path creation\n","            save_dir_name = str(ARCTOCEPHALUS_GALAPAGOENSIS_TAXON_KEY) if image_label_column == 'taxonKey' else 'Arctocephalus_galapagoensis'\n","            final_image_save_dir = os.path.join(image_output_dir, save_dir_name)\n","        else:\n","            final_image_save_dir = os.path.join(image_output_dir, 'unlabeled_images') # Fallback\n","\n","        os.makedirs(final_image_save_dir, exist_ok=True)\n","        print(f\"Images will be saved to: {final_image_save_dir}\")\n","\n","        # --- Function to download a single image ---\n","        def download_image(row):\n","            image_url = row['identifier']\n","            image_name = os.path.basename(image_url).split('?')[0]\n","            if '.' not in image_name: # Add a default extension if none present\n","                image_name += '.jpg'\n","\n","            filepath = os.path.join(final_image_save_dir, image_name)\n","\n","            if os.path.exists(filepath):\n","                return f\"Skipped: {filepath} already exists.\"\n","\n","            try:\n","                response = requests.get(image_url, stream=True, timeout=15)\n","                response.raise_for_status()\n","\n","                with open(filepath, 'wb') as f:\n","                    for chunk in response.iter_content(chunk_size=8192):\n","                        f.write(chunk)\n","                return f\"Downloaded: {filepath}\"\n","            except requests.exceptions.Timeout:\n","                return f\"Failed to download {image_url}: Timeout.\"\n","            except requests.exceptions.ConnectionError:\n","                return f\"Failed to download {image_url}: Connection error.\"\n","            except requests.exceptions.RequestException as e:\n","                status_code = response.status_code if 'response' in locals() else 'N/A'\n","                return f\"Failed to download {image_url}: HTTP Error {status_code} - {e}\"\n","            except Exception as e:\n","                return f\"An unexpected error occurred for {image_url}: {e}\"\n","\n","        # --- Concurrent downloading ---\n","        max_workers = 10\n","\n","        print(f\"\\nStarting concurrent image download with {max_workers} workers...\")\n","        results = []\n","        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","            futures = [executor.submit(download_image, row) for index, row in image_data.iterrows()]\n","\n","            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading Images\"):\n","                results.append(future.result())\n","\n","        print(\"\\nImage download process complete. Sample results:\")\n","        for res in results[:min(5, len(results))]:\n","            print(res)\n","        if len(results) > 5:\n","            print(\"...\")\n","\n","        downloaded_count = sum(1 for r in results if r.startswith(\"Downloaded:\"))\n","        skipped_count = sum(1 for r in results if r.startswith(\"Skipped:\"))\n","        failed_count = len(results) - downloaded_count - skipped_count\n","\n","        print(f\"\\nSummary:\")\n","        print(f\"  Successfully downloaded: {downloaded_count}\")\n","        print(f\"  Skipped (already exists): {skipped_count}\")\n","        print(f\"  Failed: {failed_count}\")\n","        print(f\"  Total URLs processed: {len(results)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":922,"referenced_widgets":["df21a39da343477a961f1d6804b0186c","7e79a5dcda684e739251df57c1958931","efd194df348b4618bb34ee804c5e11ee","a6e5294d96d142389576289c6f00a29b","b0acc0b50d4045efae4efab460532dab","6a350894c6744d8b9866cd8cbc332dfd","b5494a39c360448fafb68a6ecec68d27","5b135e2b213447af894ab7cfa0aecc98","9695d134a21d4009bc54bf92fd332531","77c3569ce2ef4111912011288ff35363","e9fd95b6c2114f6796ffe21064cd8dd0"]},"id":"q_pvC0Y4kldA","executionInfo":{"status":"ok","timestamp":1747776127586,"user_tz":300,"elapsed":10140,"user":{"displayName":"MICHAEL ESTEBAN QUINA MOLINA","userId":"08220967149932687746"}},"outputId":"45709548-40f1-4637-d892-765c8142dc62"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Dataset will be processed in: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A\n","Downloading DwC-A to: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/Arctocephalus_galapagoensis_dataset.zip\n","--2025-05-20 21:21:53--  https://api.gbif.org/v1/occurrence/download/request/0010447-250515123054153.zip\n","Resolving api.gbif.org (api.gbif.org)... 130.225.43.2\n","Connecting to api.gbif.org (api.gbif.org)|130.225.43.2|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://occurrence-download.gbif.org/occurrence/download/request/0010447-250515123054153.zip [following]\n","--2025-05-20 21:21:53--  https://occurrence-download.gbif.org/occurrence/download/request/0010447-250515123054153.zip\n","Resolving occurrence-download.gbif.org (occurrence-download.gbif.org)... 130.225.43.36\n","Connecting to occurrence-download.gbif.org (occurrence-download.gbif.org)|130.225.43.36|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/octet-stream]\n","Saving to: ‘/content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/Arctocephalus_galapagoensis_dataset.zip’\n","\n","/content/drive/My D     [   <=>              ] 243.00K   432KB/s    in 0.6s    \n","\n","2025-05-20 21:21:54 (432 KB/s) - ‘/content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/Arctocephalus_galapagoensis_dataset.zip’ saved [248834]\n","\n","\n","DwC-A download complete. Starting extraction...\n","DwC-A extracted successfully.\n","Loaded 596 records from multimedia.txt\n","\n","Columns available in multimedia.txt:\n","['gbifID', 'type', 'format', 'identifier', 'references', 'title', 'description', 'source', 'audience', 'created', 'creator', 'contributor', 'publisher', 'license', 'rightsHolder']\n","\n","Warning: Neither 'taxonKey' nor 'scientificName' found in multimedia.txt.\n","Images will be saved to a single folder without species-specific subfolders.\n","Found 592 'StillImage' URLs (no specific taxonKey filter applied).\n","Images will be saved to: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/images/unlabeled_images\n","\n","Starting concurrent image download with 10 workers...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading Images:   0%|          | 0/592 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df21a39da343477a961f1d6804b0186c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Image download process complete. Sample results:\n","Downloaded: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/images/unlabeled_images/original.jpg\n","Skipped: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/images/unlabeled_images/original.jpg already exists.\n","Downloaded: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/images/unlabeled_images/original.jpg\n","Downloaded: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/images/unlabeled_images/original.jpg\n","Downloaded: /content/drive/My Drive/gbif_raw_downloads2/Arctocephalus_galapagoensis_DwC-A/images/unlabeled_images/original.jpg\n","...\n","\n","Summary:\n","  Successfully downloaded: 72\n","  Skipped (already exists): 520\n","  Failed: 0\n","  Total URLs processed: 592\n"]}]},{"cell_type":"code","source":["# Step 1: Mount Google Drive (if not already mounted)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Define your download directory in Drive\n","import os\n","import zipfile\n","import pandas as pd\n","import requests\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from tqdm.notebook import tqdm # For a nice progress bar\n","\n","# Define the main directory in your Drive for all GBIF downloads\n","gbif_root_dir = '/content/drive/My Drive/gbif_raw_downloads'\n","os.makedirs(gbif_root_dir, exist_ok=True)\n","\n","# Define a specific directory for the Zalophus wollebaeki dataset\n","dataset_name = 'Zalophus_wollebaeki_DwC-A'\n","dataset_drive_path = os.path.join(gbif_root_dir, dataset_name)\n","os.makedirs(dataset_drive_path, exist_ok=True)\n","print(f\"Dataset will be processed in: {dataset_drive_path}\")\n","\n","# --- IMPORTANT: PASTE YOUR NEW GBIF DOWNLOAD LINK FOR ZALOPHUS WOLLEBAEKI HERE ---\n","GBIF_DOWNLOAD_URL = 'https://api.gbif.org/v1/occurrence/download/request/0010490-250515123054153.zip'\n","# Example: GBIF_DOWNLOAD_URL = 'https://api.gbif.org/v1/occurrence/download/request/0012345-250502131345914.zip'\n","\n","\n","dwca_filename = os.path.join(dataset_drive_path, 'Zalophus_wollebaeki_dataset.zip')\n","\n","print(f\"Downloading DwC-A to: {dwca_filename}\")\n","# Download the zip file directly to your Drive\n","!wget -O \"{dwca_filename}\" \"{GBIF_DOWNLOAD_URL}\"\n","\n","print(\"\\nDwC-A download complete. Starting extraction...\")\n","\n","# Unzip the downloaded DwC-A directly in Drive\n","try:\n","    with zipfile.ZipFile(dwca_filename, 'r') as zip_ref:\n","        zip_ref.extractall(dataset_drive_path)\n","    print(\"DwC-A extracted successfully.\")\n","except zipfile.BadZipFile:\n","    print(\"Error: The downloaded file is not a valid zip file. Please check the GBIF_DOWNLOAD_URL.\")\n","    exit()\n","except Exception as e:\n","    print(f\"An error occurred during unzipping: {e}\")\n","    exit()\n","\n","# Path to the multimedia.txt file within the extracted archive\n","multimedia_file = os.path.join(dataset_drive_path, 'multimedia.txt')\n","\n","if not os.path.exists(multimedia_file):\n","    print(f\"Error: {multimedia_file} not found. Ensure the DwC-A contained it.\")\n","    print(\"This usually means no media records were found for your search criteria on GBIF.\")\n","else:\n","    # Read the multimedia.txt file into a Pandas DataFrame\n","    multimedia_df = pd.read_csv(multimedia_file, sep='\\t', low_memory=False)\n","\n","    print(f\"Loaded {len(multimedia_df)} records from multimedia.txt\")\n","    print(\"\\nColumns available in multimedia.txt:\")\n","    print(multimedia_df.columns.tolist()) # <--- IMPORTANT: Print available columns\n","\n","    # Determine which column to use for organizing images\n","    image_label_column = None\n","    if 'taxonKey' in multimedia_df.columns:\n","        image_label_column = 'taxonKey'\n","    elif 'scientificName' in multimedia_df.columns:\n","        image_label_column = 'scientificName'\n","    else:\n","        print(\"\\nWarning: Neither 'taxonKey' nor 'scientificName' found in multimedia.txt.\")\n","        print(\"Images will be saved to a single folder without species-specific subfolders.\")\n","\n","    # Filter for 'StillImage' (photos) and relevant columns\n","    required_cols = ['identifier', 'license', 'type']\n","    if image_label_column:\n","        required_cols.append(image_label_column)\n","\n","    # Ensure all required columns exist before selecting\n","    missing_cols = [col for col in required_cols if col not in multimedia_df.columns]\n","    if missing_cols:\n","        print(f\"Error: Missing required columns in multimedia.txt: {missing_cols}\")\n","        print(\"Please check your GBIF download settings or the file content.\")\n","        exit()\n","\n","    image_data = multimedia_df[\n","        (multimedia_df['type'] == 'StillImage') &\n","        (multimedia_df['identifier'].notna())\n","    ][required_cols].dropna(subset=['identifier'])\n","\n","    # Filter to only keep images for the specific taxonKey if 'taxonKey' is present\n","    # This is the taxonKey for Zalophus wollebaeki\n","    ZALOPHUS_WOLLEBAEKI_TAXON_KEY = 5218765\n","    if image_label_column == 'taxonKey':\n","        image_data = image_data[image_data['taxonKey'] == ZALOPHUS_WOLLEBAEKI_TAXON_KEY]\n","        print(f\"Found {len(image_data)} 'StillImage' URLs for TaxonKey {ZALOPHUS_WOLLEBAEKI_TAXON_KEY}.\")\n","    elif image_label_column == 'scientificName':\n","        image_data = image_data[image_data['scientificName'] == 'Zalophus wollebaeki']\n","        print(f\"Found {len(image_data)} 'StillImage' URLs for scientificName 'Zalophus wollebaeki'.\")\n","    else:\n","        print(f\"Found {len(image_data)} 'StillImage' URLs (no specific taxonKey/scientificName filter applied).\")\n","\n","\n","    if len(image_data) == 0:\n","        print(\"No images found matching your criteria in the multimedia.txt file.\")\n","    else:\n","        # Create a directory to store the actual image files\n","        image_output_dir = os.path.join(dataset_drive_path, 'images')\n","        os.makedirs(image_output_dir, exist_ok=True)\n","\n","        # Define the final image saving directory\n","        if image_label_column:\n","            # Create a subfolder based on taxonKey or scientificName\n","            save_dir_name = str(ZALOPHUS_WOLLEBAEKI_TAXON_KEY) if image_label_column == 'taxonKey' else 'Zalophus_wollebaeki'\n","            final_image_save_dir = os.path.join(image_output_dir, save_dir_name)\n","        else:\n","            final_image_save_dir = os.path.join(image_output_dir, 'unlabeled_images') # Fallback\n","\n","        os.makedirs(final_image_save_dir, exist_ok=True)\n","        print(f\"Images will be saved to: {final_image_save_dir}\")\n","\n","        # --- Function to download a single image ---\n","        def download_image(row):\n","            image_url = row['identifier']\n","            image_name = os.path.basename(image_url).split('?')[0]\n","            if '.' not in image_name:\n","                image_name += '.jpg'\n","\n","            filepath = os.path.join(final_image_save_dir, image_name)\n","\n","            if os.path.exists(filepath):\n","                return f\"Skipped: {filepath} already exists.\"\n","\n","            try:\n","                response = requests.get(image_url, stream=True, timeout=15)\n","                response.raise_for_status()\n","\n","                with open(filepath, 'wb') as f:\n","                    for chunk in response.iter_content(chunk_size=8192):\n","                        f.write(chunk)\n","                return f\"Downloaded: {filepath}\"\n","            except requests.exceptions.Timeout:\n","                return f\"Failed to download {image_url}: Timeout.\"\n","            except requests.exceptions.ConnectionError:\n","                return f\"Failed to download {image_url}: Connection error.\"\n","            except requests.exceptions.RequestException as e:\n","                status_code = response.status_code if 'response' in locals() else 'N/A'\n","                return f\"Failed to download {image_url}: HTTP Error {status_code} - {e}\"\n","            except Exception as e:\n","                return f\"An unexpected error occurred for {image_url}: {e}\"\n","\n","        # --- Concurrent downloading ---\n","        max_workers = 10\n","\n","        print(f\"\\nStarting concurrent image download with {max_workers} workers...\")\n","        results = []\n","        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","            futures = [executor.submit(download_image, row) for index, row in image_data.iterrows()]\n","\n","            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading Images\"):\n","                results.append(future.result())\n","\n","        print(\"\\nImage download process complete. Sample results:\")\n","        for res in results[:min(5, len(results))]:\n","            print(res)\n","        if len(results) > 5:\n","            print(\"...\")\n","\n","        downloaded_count = sum(1 for r in results if r.startswith(\"Downloaded:\"))\n","        skipped_count = sum(1 for r in results if r.startswith(\"Skipped:\"))\n","        failed_count = len(results) - downloaded_count - skipped_count\n","\n","        print(f\"\\nSummary:\")\n","        print(f\"  Successfully downloaded: {downloaded_count}\")\n","        print(f\"  Skipped (already exists): {skipped_count}\")\n","        print(f\"  Failed: {failed_count}\")\n","        print(f\"  Total URLs processed: {len(results)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":922,"referenced_widgets":["362092aa05ed42d595bd169e3cd12ef1","d91b7115b9cc45db802f423b27e690a4","b370744d19a44870bba58034fa2c1356","ed40693bd152422b85a2302f2fa898eb","1c836158cda34019917de085d00be4f0","0dfcbebf45c547c78c9ae89b0557aeca","fcf83ecb71e347d99fdde23781965a13","ce99d00526664b8badd83be183f1e312","8ebfe9b88f234007ab505b82bfd49e84","e60271962da34a50ade1df25b7243cc7","6c315d3173aa41f7b72c1da1eda7c7c1"]},"id":"0qIEKMMrkxnS","executionInfo":{"status":"ok","timestamp":1747777735932,"user_tz":300,"elapsed":35182,"user":{"displayName":"MICHAEL ESTEBAN QUINA MOLINA","userId":"08220967149932687746"}},"outputId":"6d3888e7-d672-4753-b5d1-c06a9fd60a72"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Dataset will be processed in: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A\n","Downloading DwC-A to: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/Zalophus_wollebaeki_dataset.zip\n","--2025-05-20 21:48:21--  https://api.gbif.org/v1/occurrence/download/request/0010490-250515123054153.zip\n","Resolving api.gbif.org (api.gbif.org)... 130.225.43.2\n","Connecting to api.gbif.org (api.gbif.org)|130.225.43.2|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://occurrence-download.gbif.org/occurrence/download/request/0010490-250515123054153.zip [following]\n","--2025-05-20 21:48:22--  https://occurrence-download.gbif.org/occurrence/download/request/0010490-250515123054153.zip\n","Resolving occurrence-download.gbif.org (occurrence-download.gbif.org)... 130.225.43.36\n","Connecting to occurrence-download.gbif.org (occurrence-download.gbif.org)|130.225.43.36|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/octet-stream]\n","Saving to: ‘/content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/Zalophus_wollebaeki_dataset.zip’\n","\n","/content/drive/My D     [    <=>             ]   1.20M  1.53MB/s    in 0.8s    \n","\n","2025-05-20 21:48:23 (1.53 MB/s) - ‘/content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/Zalophus_wollebaeki_dataset.zip’ saved [1256704]\n","\n","\n","DwC-A download complete. Starting extraction...\n","DwC-A extracted successfully.\n","Loaded 6865 records from multimedia.txt\n","\n","Columns available in multimedia.txt:\n","['gbifID', 'type', 'format', 'identifier', 'references', 'title', 'description', 'source', 'audience', 'created', 'creator', 'contributor', 'publisher', 'license', 'rightsHolder']\n","\n","Warning: Neither 'taxonKey' nor 'scientificName' found in multimedia.txt.\n","Images will be saved to a single folder without species-specific subfolders.\n","Found 6793 'StillImage' URLs (no specific taxonKey/scientificName filter applied).\n","Images will be saved to: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/images/unlabeled_images\n","\n","Starting concurrent image download with 10 workers...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading Images:   0%|          | 0/6793 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"362092aa05ed42d595bd169e3cd12ef1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Image download process complete. Sample results:\n","Skipped: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/images/unlabeled_images/original.jpeg already exists.\n","Skipped: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/images/unlabeled_images/original.jpg already exists.\n","Skipped: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/images/unlabeled_images/original.jpg already exists.\n","Skipped: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/images/unlabeled_images/original.jpeg already exists.\n","Skipped: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/images/unlabeled_images/original.jpeg already exists.\n","...\n","\n","Summary:\n","  Successfully downloaded: 326\n","  Skipped (already exists): 6467\n","  Failed: 0\n","  Total URLs processed: 6793\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","from sklearn.model_selection import train_test_split # A convenient way to split data\n","\n","# Ensure Google Drive is mounted\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","print(\"Starting dataset organization...\")\n","\n","# --- 1. Define Paths and Class Mappings (UPDATED SOURCE PATHS) ---\n","\n","# Base directory where your downloaded images are\n","base_source_dir = '/content/drive/My Drive/gbif_raw_downloads/'\n","\n","# Mapping of taxonKey (used for organization) to human-readable class name\n","# The 'source_path' now points to the 'unlabeled_images' folder within each species' download\n","species_info = {\n","    '2433473': { # TaxonKey for Arctocephalus galapagoensis\n","        'name': 'Arctocephalus_galapagoensis',\n","        'source_path': os.path.join(base_source_dir, 'Arctocephalus_galapagoensis_DwC-A', 'images', 'unlabeled_images')\n","    },\n","    '5218765': { # TaxonKey for Zalophus wollebaeki\n","        'name': 'Zalophus_wollebaeki',\n","        'source_path': os.path.join(base_source_dir, 'Zalophus_wollebaeki_DwC-A', 'images', 'unlabeled_images')\n","    }\n","}\n","\n","# New organized dataset base directory in your Drive\n","organized_dataset_base_dir = '/content/drive/My Drive/my_galapagos_seals_dataset'\n","os.makedirs(organized_dataset_base_dir, exist_ok=True)\n","\n","\n","# --- 2. Define Split Ratios ---\n","train_split_ratio = 0.8\n","val_split_ratio = 0.1\n","test_split_ratio = 0.1\n","\n","\n","# --- 3. Create the New Dataset Structure ---\n","subdirs = ['train', 'validation', 'test']\n","\n","for subdir in subdirs:\n","    for species_key, info in species_info.items():\n","        class_name = info['name']\n","        target_path = os.path.join(organized_dataset_base_dir, subdir, class_name)\n","        os.makedirs(target_path, exist_ok=True)\n","        print(f\"Created directory: {target_path}\")\n","\n","print(\"\\nNew dataset directory structure created.\")\n","\n","# --- 4. Populate the Dataset ---\n","\n","print(\"\\nStarting to copy and split images...\")\n","\n","for species_key, info in species_info.items():\n","    species_name = info['name']\n","    source_folder = info['source_path']\n","    print(f\"\\nProcessing {species_name} from: {source_folder}\")\n","\n","    # Get all image file paths for the current species\n","    image_files = [f for f in os.listdir(source_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff'))]\n","    random.shuffle(image_files) # Shuffle to ensure random split\n","\n","    total_images = len(image_files)\n","    print(f\"Found {total_images} images for {species_name}.\")\n","\n","    if total_images == 0:\n","        print(f\"No images found in {source_folder}. Skipping {species_name}.\")\n","        continue\n","\n","    # Perform the split using sklearn's train_test_split\n","    # First split: train vs. (validation + test)\n","    train_files, val_test_files = train_test_split(\n","        image_files,\n","        train_size=train_split_ratio,\n","        random_state=42, # For reproducibility\n","        shuffle=True\n","    )\n","\n","    # Second split: validation vs. test from the remaining files\n","    if len(val_test_files) > 0:\n","        # Calculate test_size relative to the remaining val_test_files\n","        test_size_relative = test_split_ratio / (val_split_ratio + test_split_ratio)\n","        val_files, test_files = train_test_split(\n","            val_test_files,\n","            test_size=test_size_relative,\n","            random_state=42,\n","            shuffle=True\n","        )\n","    else:\n","        val_files = []\n","        test_files = []\n","\n","    print(f\"  Train: {len(train_files)} images\")\n","    print(f\"  Validation: {len(val_files)} images\")\n","    print(f\"  Test: {len(test_files)} images\")\n","\n","\n","    # Copy files to their respective directories\n","    for filename in train_files:\n","        src = os.path.join(source_folder, filename)\n","        dst = os.path.join(organized_dataset_base_dir, 'train', species_name, filename)\n","        shutil.copyfile(src, dst)\n","\n","    for filename in val_files:\n","        src = os.path.join(source_folder, filename)\n","        dst = os.path.join(organized_dataset_base_dir, 'validation', species_name, filename)\n","        shutil.copyfile(src, dst)\n","\n","    for filename in test_files:\n","        src = os.path.join(source_folder, filename)\n","        dst = os.path.join(organized_dataset_base_dir, 'test', species_name, filename)\n","        shutil.copyfile(src, dst)\n","\n","print(\"\\nDataset organization complete!\")\n","\n","# --- 5. Verify Counts (Optional but Recommended) ---\n","print(\"\\nVerifying final counts in the new dataset structure:\")\n","for subdir in subdirs:\n","    print(f\"\\n--- {subdir.upper()} Set ---\")\n","    for species_key, info in species_info.items():\n","        species_name = info['name']\n","        target_path = os.path.join(organized_dataset_base_dir, subdir, species_name)\n","        count = len(os.listdir(target_path))\n","        print(f\"  {species_name}: {count} images\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xa5stVkEq0K7","executionInfo":{"status":"ok","timestamp":1747778881273,"user_tz":300,"elapsed":13210,"user":{"displayName":"MICHAEL ESTEBAN QUINA MOLINA","userId":"08220967149932687746"}},"outputId":"2fcd51b7-4f77-4592-f018-f72595b3a7b3"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Starting dataset organization...\n","Created directory: /content/drive/My Drive/my_galapagos_seals_dataset/train/Arctocephalus_galapagoensis\n","Created directory: /content/drive/My Drive/my_galapagos_seals_dataset/train/Zalophus_wollebaeki\n","Created directory: /content/drive/My Drive/my_galapagos_seals_dataset/validation/Arctocephalus_galapagoensis\n","Created directory: /content/drive/My Drive/my_galapagos_seals_dataset/validation/Zalophus_wollebaeki\n","Created directory: /content/drive/My Drive/my_galapagos_seals_dataset/test/Arctocephalus_galapagoensis\n","Created directory: /content/drive/My Drive/my_galapagos_seals_dataset/test/Zalophus_wollebaeki\n","\n","New dataset directory structure created.\n","\n","Starting to copy and split images...\n","\n","Processing Arctocephalus_galapagoensis from: /content/drive/My Drive/gbif_raw_downloads/Arctocephalus_galapagoensis_DwC-A/images/unlabeled_images\n","Found 52 images for Arctocephalus_galapagoensis.\n","  Train: 41 images\n","  Validation: 5 images\n","  Test: 6 images\n","\n","Processing Zalophus_wollebaeki from: /content/drive/My Drive/gbif_raw_downloads/Zalophus_wollebaeki_DwC-A/images/unlabeled_images\n","Found 300 images for Zalophus_wollebaeki.\n","  Train: 240 images\n","  Validation: 30 images\n","  Test: 30 images\n","\n","Dataset organization complete!\n","\n","Verifying final counts in the new dataset structure:\n","\n","--- TRAIN Set ---\n","  Arctocephalus_galapagoensis: 41 images\n","  Zalophus_wollebaeki: 240 images\n","\n","--- VALIDATION Set ---\n","  Arctocephalus_galapagoensis: 5 images\n","  Zalophus_wollebaeki: 30 images\n","\n","--- TEST Set ---\n","  Arctocephalus_galapagoensis: 6 images\n","  Zalophus_wollebaeki: 30 images\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import os\n","\n","print(\"TensorFlow Version:\", tf.__version__)\n","\n","# Ensure Google Drive is mounted\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","print(\"Starting data loading and augmentation setup...\")\n","\n","# --- 1. Define Dataset Paths and Parameters ---\n","\n","# Base directory where your organized dataset is located\n","organized_dataset_base_dir = '/content/drive/My Drive/my_galapagos_seals_dataset'\n","\n","# Image parameters\n","IMG_HEIGHT = 224 # Common size for many pre-trained models (e.g., MobileNetV2, ResNet)\n","IMG_WIDTH = 224  # Keep height and width consistent\n","BATCH_SIZE = 32  # Number of images to process at once during training\n","\n","# --- 2. Load Datasets using image_dataset_from_directory ---\n","\n","# Load Training Data\n","print(\"\\nLoading Training Dataset...\")\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","    directory=os.path.join(organized_dataset_base_dir, 'train'),\n","    labels='inferred',       # Labels are inferred from the directory structure (folder names)\n","    label_mode='int',        # Labels will be integers (0, 1, ...)\n","    image_size=(IMG_HEIGHT, IMG_WIDTH), # Resize images to this uniform size\n","    interpolation='nearest', # Interpolation method for resizing\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,            # Shuffle training data\n","    seed=123                 # For reproducibility\n",")\n","\n","# Load Validation Data\n","print(\"Loading Validation Dataset...\")\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","    directory=os.path.join(organized_dataset_base_dir, 'validation'),\n","    labels='inferred',\n","    label_mode='int',\n","    image_size=(IMG_HEIGHT, IMG_WIDTH),\n","    interpolation='nearest',\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,           # No need to shuffle validation data\n","    seed=123\n",")\n","\n","# Load Test Data\n","print(\"Loading Test Dataset...\")\n","test_ds = tf.keras.utils.image_dataset_from_directory(\n","    directory=os.path.join(organized_dataset_base_dir, 'test'),\n","    labels='inferred',\n","    label_mode='int',\n","    image_size=(IMG_HEIGHT, IMG_WIDTH),\n","    interpolation='nearest',\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,           # No need to shuffle test data\n","    seed=123\n",")\n","\n","# Get class names (inferred from folder names)\n","class_names = train_ds.class_names\n","print(f\"\\nDiscovered Class Names: {class_names}\")\n","print(f\"Number of Classes: {len(class_names)}\")\n","\n","\n","# --- 3. Preprocessing (Rescaling Pixels) ---\n","\n","# All Keras Applications models expect pixel values in `[0, 1]` or `[-1, 1]` range.\n","# For simplicity, we'll scale to `[0, 1]` here.\n","# Some pre-trained models have their own `preprocess_input` function if they expect `[-1, 1]`.\n","# We'll apply this as part of our data pipeline.\n","\n","# Scaling factor to convert pixel values from [0, 255] to [0, 1]\n","normalization_layer = layers.Rescaling(1./255)\n","\n","# Apply normalization to all datasets\n","# The .map() method applies a function to each element of the dataset\n","train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n","test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n","\n","print(\"Images normalized to [0, 1] range.\")\n","\n","\n","# --- 4. Data Augmentation (Applied ONLY to Training Data) ---\n","\n","# Define the augmentation layers\n","# These layers are applied randomly during training to each image\n","data_augmentation = keras.Sequential([\n","    layers.RandomFlip(\"horizontal\"),        # Randomly flip images horizontally\n","    layers.RandomRotation(0.1),             # Randomly rotate images by up to 10% (36 degrees)\n","    layers.RandomZoom(0.1),                 # Randomly zoom in/out by up to 10%\n","    layers.RandomContrast(0.2),             # Randomly adjust contrast\n","    # layers.RandomTranslation(height_factor=0.1, width_factor=0.1), # Randomly shift images\n","    # layers.RandomBrightness(factor=0.2), # Randomly adjust brightness\n","], name=\"data_augmentation\")\n","\n","print(\"\\nData augmentation pipeline defined.\")\n","print(\"Example augmentation layers: RandomFlip, RandomRotation, RandomZoom, RandomContrast.\")\n","\n","\n","# Apply augmentation to the training dataset\n","# We put augmentation AFTER normalization in the pipeline for consistency.\n","# However, for performance, it can sometimes be beneficial to apply augmentation on the GPU.\n","# In TensorFlow 2.x, these layers run on the CPU by default, which is usually fine.\n","train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n","                        num_parallel_calls=tf.data.AUTOTUNE)\n","\n","print(\"Data augmentation applied to the training dataset.\")\n","\n","\n","# --- 5. Configure Datasets for Performance ---\n","\n","# Use .cache() to keep images in memory after the first epoch, speeding up subsequent epochs.\n","# Use .prefetch() to overlap data preprocessing and model execution.\n","# AUTOTUNE lets TensorFlow determine the optimal buffer size.\n","train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","print(\"\\nDatasets configured for optimal performance (caching and prefetching).\")\n","\n","# --- Verify a Batch (Optional) ---\n","# You can uncomment the following lines to inspect a batch of images and labels\n","\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# for images, labels in train_ds.take(1): # Take one batch from the training dataset\n","#     plt.figure(figsize=(10, 10))\n","#     for i in range(min(9, len(images))): # Display up to 9 images\n","#         ax = plt.subplot(3, 3, i + 1)\n","#         # Denormalize for display if necessary, or just display [0,1] images\n","#         plt.imshow(images[i].numpy())\n","#         plt.title(class_names[labels[i]])\n","#         plt.axis(\"off\")\n","#     plt.show()\n","\n","print(\"\\nDataset loading and augmentation setup complete!\")\n","print(\"Your datasets (train_ds, val_ds, test_ds) are now ready for model training.\")\n","print(\"Next, we can look at building your neural network using transfer learning.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XaCftXr5vRKD","executionInfo":{"status":"ok","timestamp":1747779590480,"user_tz":300,"elapsed":11494,"user":{"displayName":"MICHAEL ESTEBAN QUINA MOLINA","userId":"08220967149932687746"}},"outputId":"8213c070-6a50-4095-ccd1-37fd7b68d6c2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow Version: 2.18.0\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Starting data loading and augmentation setup...\n","\n","Loading Training Dataset...\n","Found 281 files belonging to 2 classes.\n","Loading Validation Dataset...\n","Found 35 files belonging to 2 classes.\n","Loading Test Dataset...\n","Found 36 files belonging to 2 classes.\n","\n","Discovered Class Names: ['Arctocephalus_galapagoensis', 'Zalophus_wollebaeki']\n","Number of Classes: 2\n","Images normalized to [0, 1] range.\n","\n","Data augmentation pipeline defined.\n","Example augmentation layers: RandomFlip, RandomRotation, RandomZoom, RandomContrast.\n","Data augmentation applied to the training dataset.\n","\n","Datasets configured for optimal performance (caching and prefetching).\n","\n","Dataset loading and augmentation setup complete!\n","Your datasets (train_ds, val_ds, test_ds) are now ready for model training.\n","Next, we can look at building your neural network using transfer learning.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","# Change: Import ResNet50V2 instead of MobileNetV2\n","from tensorflow.keras.applications import ResNet50V2\n","from tensorflow.keras.models import Model\n","import os\n","\n","print(\"TensorFlow Version:\", tf.__version__)\n","\n","# Ensure Google Drive is mounted (if not already)\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True) # Force remount just in case\n","\n","print(\"\\nStarting neural network model building with Transfer Learning (using ResNet50V2)...\")\n","\n","# --- Re-define parameters from previous step (important for model input) ---\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","BATCH_SIZE = 32\n","num_classes = 2 # We have 2 classes: Arctocephalus_galapagoensis, Zalophus_wollebaeki\n","\n","# --- Load the pre-trained base model (ResNet50V2) ---\n","# Change: Use ResNet50V2\n","base_model = ResNet50V2(\n","    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), # 3 for RGB channels\n","    include_top=False, # Remove the original classification head\n","    weights='imagenet' # Load weights pre-trained on ImageNet\n",")\n","\n","print(f\"\\nBase model ({base_model.name}) loaded successfully.\")\n","print(f\"Base model output shape: {base_model.output_shape}\")\n","\n","# --- Freeze the base model ---\n","base_model.trainable = False\n","print(f\"Base model layers set to non-trainable.\")\n","\n","# --- Build the custom classification head ---\n","inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n","\n","# Change: Use ResNet V2 specific preprocessing function\n","# This layer will scale input pixels to the range expected by ResNet50V2\n","x = tf.keras.applications.resnet_v2.preprocess_input(inputs)\n","\n","# Pass the preprocessed inputs through the base model\n","# training=False ensures base model layers (like BatchNorm) run in inference mode\n","x = base_model(x, training=False)\n","\n","# Add a GlobalAveragePooling2D layer to flatten the feature maps\n","x = layers.GlobalAveragePooling2D()(x)\n","\n","# Add a Dense layer for classification\n","x = layers.Dropout(0.2)(x) # Dropout to prevent overfitting\n","\n","# Output layer for 2 classes with 'softmax' activation\n","outputs = layers.Dense(num_classes, activation='softmax')(x)\n","\n","# Create the full model\n","model = Model(inputs, outputs)\n","\n","print(\"\\nCustom classification head built and attached to the base model.\")\n","\n","# --- Compile the model ---\n","model.compile(\n","    optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Small learning rate for transfer learning\n","    loss=keras.losses.SparseCategoricalCrossentropy(), # For integer labels\n","    metrics=['accuracy']\n",")\n","\n","print(\"\\nModel compiled successfully.\")\n","\n","# --- Display Model Summary ---\n","model.summary()\n","\n","print(\"\\nNeural network model setup complete with ResNet50V2!\")\n","print(\"Your model is now ready for training.\")\n","print(\"Next, we can proceed to train the model on your prepared datasets.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":696},"id":"qGtaxKrqx-t6","executionInfo":{"status":"ok","timestamp":1747779963507,"user_tz":300,"elapsed":6755,"user":{"displayName":"MICHAEL ESTEBAN QUINA MOLINA","userId":"08220967149932687746"}},"outputId":"bbe07e05-5f57-4c3d-e4a2-51e5f331a352"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow Version: 2.18.0\n","Mounted at /content/drive\n","\n","Starting neural network model building with Transfer Learning (using ResNet50V2)...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m94668760/94668760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","\n","Base model (resnet50v2) loaded successfully.\n","Base model output shape: (None, 7, 7, 2048)\n","Base model layers set to non-trainable.\n","\n","Custom classification head built and attached to the base model.\n","\n","Model compiled successfully.\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ resnet50v2 (\u001b[38;5;33mFunctional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,564,800\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m4,098\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ resnet50v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,568,898\u001b[0m (89.91 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,568,898</span> (89.91 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,098\u001b[0m (16.01 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> (16.01 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,564,800\u001b[0m (89.89 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> (89.89 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Neural network model setup complete with ResNet50V2!\n","Your model is now ready for training.\n","Next, we can proceed to train the model on your prepared datasets.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import os\n","\n","print(\"TensorFlow Version:\", tf.__version__)\n","\n","# Ensure Google Drive is mounted (if not already)\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","print(\"\\nStarting model training...\")\n","\n","# --- Define Training Parameters ---\n","EPOCHS = 30 # Number of times to iterate over the entire training dataset\n","            # This is an initial guess; EarlyStopping will help prevent overfitting.\n","\n","# --- Define Callbacks ---\n","\n","# Directory to save your trained models\n","model_save_dir = '/content/drive/My Drive/my_galapagos_seals_model'\n","os.makedirs(model_save_dir, exist_ok=True) # Create the directory if it doesn't exist\n","\n","# Model Checkpoint: Saves the best model based on validation accuracy\n","checkpoint_filepath = os.path.join(model_save_dir, 'best_galapagos_seals_resnet_model.h5')\n","model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=False, # Save the entire model\n","    monitor='val_accuracy',  # Monitor validation accuracy\n","    mode='max',              # We want to maximize validation accuracy\n","    save_best_only=True,     # Only save when it's the best so far\n","    verbose=1                # Print messages when saving\n",")\n","\n","# Early Stopping: Stops training if validation accuracy doesn't improve\n","early_stopping_callback = keras.callbacks.EarlyStopping(\n","    monitor='val_accuracy',\n","    mode='max',\n","    patience=10,             # Number of epochs with no improvement after which training will be stopped.\n","    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity.\n","    verbose=1\n",")\n","\n","# List of callbacks to use during training\n","callbacks_list = [model_checkpoint_callback, early_stopping_callback]\n","\n","# --- Train the Model ---\n","\n","print(f\"\\nTraining for {EPOCHS} epochs with patience of {early_stopping_callback.patience}...\")\n","print(f\"Best model will be saved to: {checkpoint_filepath}\")\n","\n","history = model.fit(\n","    train_ds,\n","    epochs=EPOCHS,\n","    validation_data=val_ds,\n","    callbacks=callbacks_list\n",")\n","\n","print(\"\\nModel training complete!\")\n","\n","# --- Evaluate the Model on the Test Set ---\n","print(\"\\nEvaluating model on the test dataset...\")\n","loss, accuracy = model.evaluate(test_ds)\n","\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","print(\"\\nTraining summary:\")\n","print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n","print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n","\n","print(\"\\nNext, you can:\")\n","print(\"1. Load the best saved model using `tf.keras.models.load_model()`.\")\n","print(\"2. Visualize training history (loss and accuracy curves).\")\n","print(\"3. Perform fine-tuning to potentially achieve higher accuracy.\")\n","print(\"4. Make predictions on new images.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzHCVKKJza9A","executionInfo":{"status":"ok","timestamp":1747781059081,"user_tz":300,"elapsed":964721,"user":{"displayName":"MICHAEL ESTEBAN QUINA MOLINA","userId":"08220967149932687746"}},"outputId":"830a7ddd-0a50-44e3-acfa-4066cfacd0f0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow Version: 2.18.0\n","Mounted at /content/drive\n","\n","Starting model training...\n","\n","Training for 30 epochs with patience of 10...\n","Best model will be saved to: /content/drive/My Drive/my_galapagos_seals_model/best_galapagos_seals_resnet_model.h5\n","Epoch 1/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.1838 - loss: 0.8981\n","Epoch 1: val_accuracy improved from -inf to 0.14286, saving model to /content/drive/My Drive/my_galapagos_seals_model/best_galapagos_seals_resnet_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7s/step - accuracy: 0.1854 - loss: 0.8962 - val_accuracy: 0.1429 - val_loss: 0.8326\n","Epoch 2/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.2311 - loss: 0.8161\n","Epoch 2: val_accuracy improved from 0.14286 to 0.20000, saving model to /content/drive/My Drive/my_galapagos_seals_model/best_galapagos_seals_resnet_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 8s/step - accuracy: 0.2339 - loss: 0.8143 - val_accuracy: 0.2000 - val_loss: 0.7508\n","Epoch 3/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.3443 - loss: 0.7392\n","Epoch 3: val_accuracy improved from 0.20000 to 0.65714, saving model to /content/drive/My Drive/my_galapagos_seals_model/best_galapagos_seals_resnet_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 7s/step - accuracy: 0.3490 - loss: 0.7377 - val_accuracy: 0.6571 - val_loss: 0.6811\n","Epoch 4/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5882 - loss: 0.6714\n","Epoch 4: val_accuracy improved from 0.65714 to 0.82857, saving model to /content/drive/My Drive/my_galapagos_seals_model/best_galapagos_seals_resnet_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 7s/step - accuracy: 0.5916 - loss: 0.6706 - val_accuracy: 0.8286 - val_loss: 0.6231\n","Epoch 5/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.7938 - loss: 0.6110\n","Epoch 5: val_accuracy improved from 0.82857 to 0.85714, saving model to /content/drive/My Drive/my_galapagos_seals_model/best_galapagos_seals_resnet_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 7s/step - accuracy: 0.7952 - loss: 0.6102 - val_accuracy: 0.8571 - val_loss: 0.5766\n","Epoch 6/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8543 - loss: 0.5563\n","Epoch 6: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6s/step - accuracy: 0.8529 - loss: 0.5568 - val_accuracy: 0.8571 - val_loss: 0.5397\n","Epoch 7/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8641 - loss: 0.5181\n","Epoch 7: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6s/step - accuracy: 0.8628 - loss: 0.5188 - val_accuracy: 0.8571 - val_loss: 0.5106\n","Epoch 8/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8679 - loss: 0.4842\n","Epoch 8: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 7s/step - accuracy: 0.8662 - loss: 0.4853 - val_accuracy: 0.8571 - val_loss: 0.4884\n","Epoch 9/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8705 - loss: 0.4815\n","Epoch 9: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 7s/step - accuracy: 0.8689 - loss: 0.4824 - val_accuracy: 0.8571 - val_loss: 0.4715\n","Epoch 10/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8705 - loss: 0.4623\n","Epoch 10: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 7s/step - accuracy: 0.8689 - loss: 0.4640 - val_accuracy: 0.8571 - val_loss: 0.4581\n","Epoch 11/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8705 - loss: 0.4382\n","Epoch 11: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 7s/step - accuracy: 0.8689 - loss: 0.4399 - val_accuracy: 0.8571 - val_loss: 0.4479\n","Epoch 12/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8705 - loss: 0.4394\n","Epoch 12: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6s/step - accuracy: 0.8689 - loss: 0.4406 - val_accuracy: 0.8571 - val_loss: 0.4401\n","Epoch 13/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8705 - loss: 0.4204\n","Epoch 13: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 7s/step - accuracy: 0.8689 - loss: 0.4219 - val_accuracy: 0.8571 - val_loss: 0.4339\n","Epoch 14/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8705 - loss: 0.4073\n","Epoch 14: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6s/step - accuracy: 0.8689 - loss: 0.4098 - val_accuracy: 0.8571 - val_loss: 0.4292\n","Epoch 15/30\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8705 - loss: 0.4048\n","Epoch 15: val_accuracy did not improve from 0.85714\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7s/step - accuracy: 0.8689 - loss: 0.4074 - val_accuracy: 0.8571 - val_loss: 0.4256\n","Epoch 15: early stopping\n","Restoring model weights from the end of the best epoch: 5.\n","\n","Model training complete!\n","\n","Evaluating model on the test dataset...\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 602ms/step - accuracy: 0.8264 - loss: 0.5894\n","Test Loss: 0.5872\n","Test Accuracy: 0.8333\n","\n","Training summary:\n","Final training accuracy: 0.8541\n","Final validation accuracy: 0.8571\n","\n","Next, you can:\n","1. Load the best saved model using `tf.keras.models.load_model()`.\n","2. Visualize training history (loss and accuracy curves).\n","3. Perform fine-tuning to potentially achieve higher accuracy.\n","4. Make predictions on new images.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5b_hWEe_z8i9"},"execution_count":null,"outputs":[]}]}